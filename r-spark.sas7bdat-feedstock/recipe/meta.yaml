{% set version = '1.4' %}

{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-spark.sas7bdat
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/spark.sas7bdat_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/spark.sas7bdat/spark.sas7bdat_{{ version }}.tar.gz
  sha256: 6afeb3f74a5c0b49069dec38cd3281a25619dfbb236d10f858ce76b9a16930e8

build:
  merge_build_host: True  # [win]
  # If this is a new build for the same version, increment the build number.
  number: 0
  # no skip
  noarch: generic

  # This is required to make R link correctly on Linux.
  rpaths:
    - lib/R/lib/
    - lib/

# Suggests: knitr, rmarkdown
requirements:
  build:
    - {{ posix }}zip               # [win]

  host:
    - r-base
    - r-sparklyr >=0.3

  run:
    - r-base
    - r-sparklyr >=0.3

test:
  commands:
    # You can put additional test commands to be run here.
    - $R -e "library('spark.sas7bdat')"           # [not win]
    - "\"%R%\" -e \"library('spark.sas7bdat')\""  # [win]

  # You can also put a file called run_test.py, run_test.sh, or run_test.bat
  # in the recipe that will be run at test time.

  # requires:
    # Put any additional test requirements here.

about:
  home: https://github.com/bnosac/spark.sas7bdat
  license: GPL-3
  summary: Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark' from R. 'Apache Spark'
    is an open source cluster computing framework available at <http://spark.apache.org>.
    This R package uses the 'spark-sas7bdat' 'Spark' package (<https://spark-packages.org/package/saurfang/spark-sas7bdat>)
    to import and process 'SAS' data in parallel using 'Spark'. Hereby allowing to execute
    'dplyr' statements in parallel on top of 'SAS' data.
  license_family: GPL3
  license_file:
    - '{{ environ["PREFIX"] }}/lib/R/share/licenses/GPL-3'

# The original CRAN metadata for this package was:

# Package: spark.sas7bdat
# Type: Package
# Title: Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark'
# Description: Read in 'SAS' Data ('.sas7bdat' Files) into 'Apache Spark' from R. 'Apache Spark' is an open source cluster computing framework available at <http://spark.apache.org>. This R package uses the 'spark-sas7bdat' 'Spark' package (<https://spark-packages.org/package/saurfang/spark-sas7bdat>) to import and process 'SAS' data in parallel using 'Spark'. Hereby allowing to execute 'dplyr' statements in parallel on top of 'SAS' data.
# Maintainer: Jan Wijffels <jwijffels@bnosac.be>
# Authors@R: c(person("Jan", "Wijffels", role = c("aut", "cre", "cph"), email = "jwijffels@bnosac.be"), person("BNOSAC", role = "cph"), person("Geyer", "Bisschoff", email = "bisschoff.geyer@gmail.com", role = "ctb"))
# License: GPL-3
# Version: 1.4
# URL: https://github.com/bnosac/spark.sas7bdat
# VignetteBuilder: knitr
# Imports: sparklyr (>= 0.3)
# Suggests: knitr, rmarkdown
# RoxygenNote: 7.1.1
# NeedsCompilation: no
# Packaged: 2021-04-19 06:58:44 UTC; Jan
# Author: Jan Wijffels [aut, cre, cph], BNOSAC [cph], Geyer Bisschoff [ctb]
# Repository: CRAN
# Date/Publication: 2021-04-19 07:30:02 UTC

# See
# https://docs.conda.io/projects/conda-build for
# more information about meta.yaml
