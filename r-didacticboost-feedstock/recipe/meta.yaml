{% set version = '0.1.1' %}

{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-didacticboost
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/DidacticBoost_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/DidacticBoost/DidacticBoost_{{ version }}.tar.gz
  sha256: 3fa3739250ff32d06787231508333be43c75b00ef1cfe788595b3a6b6c30307c

build:
  merge_build_host: True  # [win]
  # If this is a new build for the same version, increment the build number.
  number: 0
  # no skip
  noarch: generic

  # This is required to make R link correctly on Linux.
  rpaths:
    - lib/R/lib/
    - lib/

# Suggests: testthat
requirements:
  build:
    - {{ posix }}zip               # [win]

  host:
    - r-base
    - r-rpart >=4.1_10

  run:
    - r-base
    - r-rpart >=4.1_10

test:
  commands:
    # You can put additional test commands to be run here.
    - $R -e "library('DidacticBoost')"           # [not win]
    - "\"%R%\" -e \"library('DidacticBoost')\""  # [win]

  # You can also put a file called run_test.py, run_test.sh, or run_test.bat
  # in the recipe that will be run at test time.

  # requires:
    # Put any additional test requirements here.

about:
  home: https://github.com/dashaub/DidacticBoost
  license: GPL-3
  summary: A basic, clear implementation of tree-based gradient boosting designed to illustrate
    the core operation of boosting models. Tuning parameters (such as stochastic subsampling,
    modified learning rate, or regularization) are not implemented. The only adjustable
    parameter is the number of training rounds. If you are looking for a high performance
    boosting implementation with tuning parameters, consider the 'xgboost' package.
  license_family: GPL3
  license_file:
    - '{{ environ["PREFIX"] }}/lib/R/share/licenses/GPL-3'

extra:
  recipe-maintainers:
    - katietz

# The original CRAN metadata for this package was:

# Package: DidacticBoost
# Type: Package
# Title: A Simple Implementation and Demonstration of Gradient Boosting
# Version: 0.1.1
# Date: 2016-04-19
# Authors@R: person("David", "Shaub", email = "davidshaub@gmx.com", role = c("aut", "cre"))
# Description: A basic, clear implementation of tree-based gradient boosting designed to illustrate the core operation of boosting models. Tuning parameters (such as stochastic subsampling, modified learning rate, or regularization) are not implemented. The only adjustable parameter is the number of training rounds. If you are looking for a high performance boosting implementation with tuning parameters, consider the 'xgboost' package.
# License: GPL-3
# Depends: R (>= 3.1.1), rpart (>= 4.1-10)
# Suggests: testthat
# URL: https://github.com/dashaub/DidacticBoost
# BugReports: https://github.com/dashaub/DidacticBoost/issues
# ByteCompile: true
# NeedsCompilation: no
# LazyData: TRUE
# RoxygenNote: 5.0.1
# Packaged: 2016-04-19 01:46:08 UTC; david
# Author: David Shaub [aut, cre]
# Maintainer: David Shaub <davidshaub@gmx.com>
# Repository: CRAN
# Date/Publication: 2016-04-19 08:11:59

# See
# https://docs.conda.io/projects/conda-build for
# more information about meta.yaml
